<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>Convolutional Neural Networks using TensorFlow and Keras</title>

    <meta name="description" content="A framework for easily creating beautiful presentations using HTML">
    <meta name="author" content="Hakim El Hattab">

    <meta name="apple-mobile-web-app-capable" content="yes"/>
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>

    <meta name="viewport"
          content="initial-scale=1.0, maximum-scale=10.0, user-scalable=yes">

    <link rel="stylesheet" href="reveal.js/css/reveal.css">
    <!--<link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">-->
    <link rel="stylesheet" href="reveal.js/css/theme/night.css" id="theme">
    <!--<link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">-->
    <!--<link rel="stylesheet" href="reveal.js/css/theme/night.css" id="theme">-->
    <!--<link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">-->


    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">
    <style>
        pre code {
            /*display: block;*/
            /*padding: 0.5em;*/
            /*background: #FFFFFF !important;*/
            /*color: #000000 !important;*/
            font-size: larger !important;
        }

        .right-img {
            margin-left: 10px !important;
            float: right;
            /*height: 500px;*/
        }
        .left-img {
            margin-left: 10px !important;
            float: left;
            /*height: 500px;*/
        }
        .todo:before {
            content: 'TODO: ';
        }
        .todo {
            color: red !important;
        }
        code span.line-number {
            color: lightcoral;
        }

        .reveal {
          -ms-touch-action: auto !important;
              touch-action: auto !important;
        }

    </style>

    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match(/print-pdf/gi) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);
    </script>

    <!--[if lt IE 9]>
    <script src="reveal.js/lib/js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>

<div class="reveal">
    <div class="slides">

        <section>
            <h3>Introduction to Convolutional Neural Networks using TensorFlow and Keras</h3>
            <h4><a href="https://pydata.org/london2017/schedule/presentation/5/" target="_blank">PyData London 2017</a></h4>
                <p><a href="http://zeigermann.eu">Oliver Zeigermann</a> / <a href="http://twitter.com/djcordhose">@DJCordhose</a>
                </p>

            <p>Slides: <a href="https://djcordhose.github.io/speed-limit-signs/2017_pydata_london.html">
                https://djcordhose.github.io/speed-limit-signs/2017_pydata_london.html</a></p>

        </section>

        <section class="todo">
        <h2>Plan for Today</h2>
<pre>
Install, 10 Minutes
- Notebooks on Azure vs local

Prep, 10 Minutes
- Image Processing
- Test / Train split

Train (Main Part), 60 Minutes
- Layers Types and their Functions
- Train Concept
- Validation
- Trying out VGG and Inception Architectures
- Saving the trained model

Usage 10 Minutes
- Loading the pre trained model
- Trying it out on real life data
</pre>
        </section>


        <section>
            <h3>Our Challenge for today: Recognizing Speed limit signs</h3>
            <ul>
                <li><a href="https://en.wikipedia.org/wiki/Traffic_sign_recognition" target="_blank">
                    built into luxury cars for almost 10 years now
                </a>
                <li>We use real images from German streets
                <li>Low Resolution, Overall bad Quality, Limited number of samples
            </ul>
            <img src="img/signs.png">
           <p><small>Big Kudos to
                <a target="_blank" href="https://medium.com/@waleedka/traffic-sign-recognition-with-tensorflow-629dffc391a6#.i728o84ib">
Traffic Sign Recognition with TensorFlow
                </a>

                by

                <a href="https://medium.com/@waleedka" target="_blank">Waleed Abdulla</a>

for providing the initial idea and many of the functions used to prepare and display the images

            </small></p>
        </section>

        <section>
            <h3>Using Jupyter Notebooks</h3>
            <p>There are three ways of following along</p>
            <ol>
                <li>Using preview of Notebooks on Github:
                    <a href="https://github.com/DJCordhose/speed-limit-signs/tree/master/notebooks/azure" target="_blank">
                        https://github.com/DJCordhose/speed-limit-signs/tree/master/notebooks/azure
                    </a> (all you need is a browser and Internet)
                </li>
                <li>Using Azure Notebooks:
                <a href="https://notebooks.azure.com" target="_blank">
                    https://notebooks.azure.com
                </a> (requires you to register on azure, but free of cost)
                <li>Local Installation (if you have been in the previous session): clone from <a href="https://github.com/DJCordhose/speed-limit-signs" target="_blank">
                        https://github.com/DJCordhose/speed-limit-signs
                    </a> (all you need is a browser and Internet)

            </ol>
        </section>

        <section>
            <h1>Main Part</h1>
        </section>

        <section>
            <h3>Deep Learning</h3>
            <div class="fragment">
            <img src="img/deep-learning.png" height="500">
            <p><small><a target="_blank" href="https://drive.google.com/file/d/0BxKBnD5y2M8NdFB4U1BtU1V1ejA/view">
            Deep Learning: Course by Yann LeCun at Coll√®ge de France 2016
            </a></small></p>
            </div>
        </section>

        <section>
            <h2>Convolutional Networks (CNN)</h2>
            <h3>special networks to process images</h3>
            <h3>using different kinds of specialized layers</h3>
            <h3>often used with pre-trained models</h3>
        </section>

        <section>
            <h3>Convolutional Networks</h3>
            <p>turning shallow, but large data into deep semantic information</p>
            <div>
                <ul class="fragment">
                    <li>some layers turn input into a number of filtered outputs
                    <li>other layers down sample images (giving them a lower resoluation)
                    <li>each layer adds semantic information
                </ul>
            </div>
            <!--<img src="conv-network.svg">-->
            <!--<p><small><a target="_blank" href="https://openai.com/blog/generative-models/">-->
            <!--https://openai.com/blog/generative-models/-->
            <!--</a></small></p>-->
            <img src="img/udacity-ud730-convolutional-pyramid.png" style="height: 350px">
            <p><small><a target="_blank" href="https://www.udacity.com/course/deep-learning--ud730">
                Udacity Course 730, Deep Learning (L3 Convolutional Neural Networks > Convolutional Networks)
            </a></small></p>
        </section>

        <section>
        <h3>Convolutions</h3>
        <div class="fragment">
        <img src="img/udc-730-convolutions.png" height="500">
        <p><small><a target="_blank" href="https://www.udacity.com/course/deep-learning--ud730">
        Udacity Course 730, Deep Learning (L3 Convolutional Neural Networks > Convolutional Networks)
        </a></small></p>
        </div>
        </section>

        <section>
            <h2>Types of Layers and their Functions</h2>
        </section>

        <section>
            <h3>Input Layer</h3>
            <pre><code data-trim class="line-numbers">
# input tensor for a 3-channel 64x64 image
inputs = Input(shape=(64, 64, 3))
            </code></pre>
            <p>Not strictly speaking a real layer, just interface to input</p>
        </section>

        <section>
            <h3>Convolutional Layer</h3>
            <pre><code data-trim class="line-numbers">
# 32 filters with a 4x4 kernel
x = Convolution2D(32, 4, 4, activation='relu')(x)
            </code></pre>
            <p>You cascade many of them having down sampling in between</p>
        </section>

        <section>
            <h3>Down Sampling Layer</h3>
            <pre><code data-trim class="line-numbers">
# max pooling with 2x2 window, reducing data to a fourth
x = MaxPooling2D(pool_size=(2, 2))(x)
            </code></pre>
            <p>Reduces data sizes and risk of overfitting</p>
        </section>

        <section>
            <h3>Dropout</h3>
            <pre><code data-trim class="line-numbers">
# drops 50 % of all connections
x = Dropout(0.50)(x)
            </code></pre>
            <p>Also reduces risk of overfitting</p>
        </section>

        <section>
            <h3>Last Two Layers are typically again standard</h3>
            <pre><code data-trim class="line-numbers">
# flatten output from many layers into one
x = Flatten()(x)
# fully connectied, 256 nodes
x = Dense(256, activation='relu')(x)

# softmax, 6 categories
predictions = Dense(6, activation='softmax')(x)
            </code></pre>
            <p>For classification and translation invariance</p>
        </section>

        <section>
            <h2>Optmizers</h2>
        </section>

        <section>
            <h3>Using RMSprop as an Optimizer</h3>
            <p><a href="http://cs231n.github.io/neural-networks-3/#ada">RMSprop</a> seems to be most advanced, but not scientifically published</p>
        </section>

        <section>
            <h3>Comparing to other optimizers</h3>
            <img src="img/opt1.gif" class="fragment" style="float: left" width="400px">
            <img src="img/opt2.gif" class="fragment" style="float: right" width="400px">
            <p><small>Kudos to
                <a href="https://twitter.com/alecrad">https://twitter.com/alecrad</a>
                </small></p>
        </section>

        <section>
            <h2>Architectures</h2>
        </section>

        <section>
            <h3>Layout of a typical CNN</h3>
            <a target="_blank" href="http://cs231n.github.io/convolutional-networks/">
                <img src="img/convnet-layoyt.jpeg">
            </a>
            <a target="_blank" href="http://cs231n.github.io/convolutional-networks/">http://cs231n.github.io/convolutional-networks/</a>
        </section>

        <section>
            <h3>VGG Architecture</h3>
            <ul>
                <li>we use a VGG like architecture
                <li>based on <a href="https://arxiv.org/abs/1409.1556" target="_blank">https://arxiv.org/abs/1409.1556</a>
                <li>basic idea: sequential, deep, small convolutional filters, use dropouts to reduce overfitting
                <li>16/19 layers are typical
            </ul>
        </section>

        <section>
            <h3>Code Sample</h3>
            <pre><code data-trim>
x = Convolution2D(32, 4, 4, activation='relu')(inputs)
x = MaxPooling2D(pool_size=(2, 2))(x)
x = Dropout(0.25)(x)

x = Convolution2D(64, 4, 4, activation='relu')(x)
x = MaxPooling2D(pool_size=(2, 2))(x)
x = Dropout(0.25)(x)

x = Convolution2D(128, 4, 4, activation='relu')(x)
x = MaxPooling2D(pool_size=(2, 2))(x)
x = Dropout(0.25)(x)
            </code></pre>
        </section>

        <section>
            <h3>Alternative Architecture #1: Inception</h3>
            <ul>
                <li>mentioned as <a href="https://keras.io/getting-started/functional-api-guide/" target="_blank">example in Keras documentation</a>
                <li>based on <a href="http://arxiv.org/abs/1409.4842" target="_blank">http://arxiv.org/abs/1409.4842</a>
                <li>basic idea: non-sequential, have different indepdent towers hat are eventually merged together
            </ul>
        </section>

        <section>
            <h3>Code Sample</h3>
            <pre><code data-trim>
tower_1 = Convolution2D(16, (1, 1), activation='relu')(inputs)
tower_1 = Convolution2D(16, (3, 3), activation='relu')(tower_1)
tower_1 = Dropout(0.5)(tower_1)

tower_2 = Convolution2D(16, (1, 1), activation='relu')(inputs)
tower_2 = Convolution2D(16, (5, 5), activation='relu')(tower_2)
tower_2 = Dropout(0.5)(tower_2)

tower_3 = MaxPooling2D((3, 3), strides=(1, 1))(inputs)
tower_3 = Convolution2D(16, (1, 1), activation='relu')(tower_3)
tower_3 = Dropout(0.5)(tower_3)

output = keras.layers.concatenate([tower_1, tower_2, tower_3])
</code></pre>
        </section>

        <section>
            <h3>Full Inception Architecture</h3>
            <img src="img/inception.png" height="480px">
            <p><small><a href="https://arxiv.org/abs/1409.4842" target="_blank">Going Deeper with Convolutions</a></small></p>
        </section>

        <section>
            <h3>Alternative Architecture #2: Residual connection on a convolution layer</h3>
            <ul>
                <li>mentioned as <a href="https://keras.io/getting-started/functional-api-guide/" target="_blank">example in Keras documentation</a>
                <li>based on <a href="http://arxiv.org/abs/1512.03385" target="_blank">http://arxiv.org/abs/1512.03385</a>
                <li>basic idea: depth does matter, 8x deeper than VGG, possible by addtionally feeding in unchanged input into deeper layers
            </ul>
        </section>

        <section>
            <h3>Code Sample</h3>
            <p>Many of those blocks, on after the other</p>
            <pre><code data-trim>
# 3x3 conv with 3 output channels (same as input channels)
y = Conv2D(3, (3, 3), activation='relu')(input)
# this returns input + y.
z = keras.layers.add([input, y])</code></pre>
            <img src="img/residual.png" height="300">
        </section>

        <section>
            <h3>Keras Applications</h3>
            <p>Keras comes with a numer of <a target="_blank" href="https://keras.io/applications">architectures</a> pre-trained with
                <a href="http://www.image-net.org/" target="_blank">ImageNet</a> database
            </p>
            <ul>
                <li><a target="_blank" href="https://keras.io/applications/#vgg16">VGG16</a>/<a target="_blank" href="https://keras.io/applications/#vgg19">VGG19</a>,
                source:
                <a href="https://github.com/fchollet/keras/blob/master/keras/applications/vgg16.py" target="_blank">
                vgg16.py</a>
                    /
                <a href="https://github.com/fchollet/keras/blob/master/keras/applications/vgg19.py" target="_blank">
                vgg19.py</a>
                <li><a target="_blank" href="https://keras.io/applications/#resnet50">ResNet50</a>,
                source: <a href="https://github.com/fchollet/keras/blob/master/keras/applications/resnet50.py" target="_blank">
                resnet50.py</a>

                <li><a target="_blank" href="https://keras.io/applications/#inceptionv3">Inception</a>,
                source: <a href="https://github.com/fchollet/keras/blob/master/keras/applications/inception_v3.py" target="_blank">
                inception_v3.py</a>
            </ul>
        </section>

        <section>
            <h1>Material</h1>
        </section>

        <section class="todo">
<pre>
- create bar chart for slides
- create local version of pydata-london notebook
- clearer description of creating the notebook on azure notebooks
- this works:
    - go to: https://notebooks.azure.com/djcordhose/libraries/pydata
    - click on workspace.ipynb
    - azure will ask you to login or create a new account
    - this automatically creates a copy of the library and starts the notebook
- deleting the clone and trying again will give an error
- try it out with the floreysoft user
- have creating from github as a fallback
- Grafiken aus ConvNet einbauen zur Illustration, was ein ConvNet tut, eigentlich ja Blackbox
  - http://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html
  - http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html
- https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model
- why relu
- provide pre trained network
</pre>
        </section>



        <section>
            <h3>Task 0: Looking at the samples and understanding the challenge</h3>
            <h4>What preparation is needed?</h4>
            <h4>Jupyter Notebook basics</h4>
            <p>We open the first notebook and go through it together</p>
                <a href="https://github.com/DJCordhose/speed-limit-signs/blob/master/notebooks/azure/pydata-london.ipynb" target="_blank">
                    https://github.com/DJCordhose/speed-limit-signs/blob/master/notebooks/azure/pydata-london.ipynb
                </a>
        </section>

        <section>
            <h3>Steps</h3>
            <ol>
                <li>Open link to notebooks: <a href="https://notebooks.azure.com" target="_blank">
                    https://notebooks.azure.com
                </a> in any browser
                <li>Either log in with your existing Microsoft Account or create a new one
                <li>Create a new library and upload the initial notebook from <a target="_blank" href="https://raw.githubusercontent.com/DJCordhose/speed-limit-signs/master/notebooks/azure/pydata-london.ipynb">
                    https://raw.githubusercontent.com/DJCordhose/speed-limit-signs/master/notebooks/azure/pydata-london.ipynb</a>
            </ol>
        </section>

        <section>
            <h3>Task 1: Train and Validate a simple Keras Model</h3>
        </section>

        <section>
            <h3>Task 2: Creating a convolutional Network</h3>
        </section>

        <section>
        <h3>Translation Invariance</h3>
        <div class="fragment">
        <img src="img/udc-730-translation-invariance.png" height="500">
        <p><small><a target="_blank" href="https://www.udacity.com/course/deep-learning--ud730">
        Udacity Course 730, Deep Learning (L3 Convolutional Neural Networks > Convolutional Networks)
        </a></small></p>
        </div>
        </section>

        <section>
            <h3>Intuition for Convolutional Networks</h3>
            <div class="fragment">
                <p>E.g. to recognize dogs (again a classification problem)</p>
                <img src="img/dogs.jpg">
                <p class="fragment">using an internal representation like</p>
                <img class="fragment" src="img/vgg_filter_05_crop.jpg">
            </div>
            <p><small><a target="_blank" href="https://auduno.github.io/2016/06/18/peeking-inside-convnets/">
                https://auduno.github.io/2016/06/18/peeking-inside-convnets/
            </a></small></p>
        </section>

        <section class="todo">
            <h2>Why does this work?</h2>
            <ol>
                <li>CNNs are mostly a black box
                <li>Understanding how they work is a moving field
                <li>Many things previously believed to be true no longer hold
            </ol>
        </section>

        <section class="todo">
            <h3>Why RELU Activation?</h3>
        </section>

        <section class="todo">
            <h3>How do we achieve translation invariance?</h3>
        </section>

        <!--<section class="todo">-->
            <!--<h3>Batch Sizes</h3>-->
        <!--</section>-->

        <section class="todo">
            <h3>Loss vs Accuracy</h3>
        </section>

        <section class="todo">
            <h3>A word on overfitting</h3>
            <ol>
                <li>we do not have a lot of data to train with
                <li>80 / 20 split for training might still be a bad idea
                <li>very likely we are overfitting just because of this
            </ol>
        </section>



   </div>

</div>

<script src="reveal.js/lib/js/head.min.js"></script>
<script src="reveal.js/js/reveal.js"></script>
<script src="lib/jquery-2.2.4.js"></script>

<script>
    $('section').attr('data-background-image', "backgrounds/dark-blur.jpg")
</script>
<script>
    Reveal.addEventListener( 'ready', function( event ) {
        if (window.location.hostname.indexOf('localhost') !== -1) {
            // only applies to presentation version
            Reveal.configure({ controls: false });
        } else {
            // only applies to public version
            $('.fragment').removeClass('fragment');
        }
        // applies to all versions
//        $('.fragment').removeClass('fragment');
    } );
</script>

<script>

    // Full list of configuration options available at:
    // https://github.com/hakimel/reveal.js#configuration
    Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,

        transition: 'fade', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
            {
                src: 'reveal.js/lib/js/classList.js', condition: function () {
                return !document.body.classList;
            }
            },
            {
                src: 'reveal.js/plugin/markdown/marked.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: 'reveal.js/plugin/markdown/markdown.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: 'reveal.js/plugin/highlight/highlight.js', async: true, condition: function () {
                return !!document.querySelector('pre code');
            }, callback: function () {
                hljs.initHighlightingOnLoad();
            }
            },
            {src: 'reveal.js/plugin/zoom-js/zoom.js', async: true},
            {src: 'reveal.js/plugin/notes/notes.js', async: true},
            {src: 'lib/js/line-numbers.js'}
        ]
    });

</script>

</body>
</html>
